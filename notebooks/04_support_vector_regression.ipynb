{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPkktvV9lXfa"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dw9xOlfqlXfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.optimize import minimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZGuj7sr4lXfd"
      },
      "outputs": [],
      "source": [
        "def get_simple():\n",
        "    X = np.linspace(-3, 3, 11)\n",
        "    y = np.sin(X)\n",
        "    y+=np.random.randn(11)*.2\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rErHJgHllXfe"
      },
      "source": [
        "### **1. Training of SVRs via Constrained Optimization** <a class=\"anchor\" id=\"optim\"></a>\n",
        "\n",
        "Throughout this notebook, we assume $\\mathbf{X} \\in \\mathbb{R}^{N \\times D}$ as $N \\times D$ matrix of training examples and $\\mathbf{t} \\in \\mathbb{R}^N$ as $N$-dimensional vector of training targets.\n",
        "To express the dual SVR in standard form, we express the kernel matrix $K \\in \\mathbb{R}^{NxN}$ such that each entry is $K_{ij} = k(\\mathbf{x}_i , \\mathbf{x}_j)$.\n",
        "\n",
        "The dual form of the SVR was introduced as:\n",
        "\n",
        "\\begin{align}\n",
        "  \\widetilde{L}(\\mathbf a,\\widehat{\\mathbf a}) =& - \\frac{1}{2}  \\sum_{n=1}^N  \\sum_{m=1}^N (a_n - \\widehat a _n) (a_m - \\widehat a _m)k(\\mathbf x_n,\\mathbf x_m)\\\\ &- \\epsilon  \\sum_{n=1}^N (a_n + \\widehat a _n) +  \\sum_{m=1}^N (a_n - \\widehat a _n) t_n\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "> To simplify the mathematical procedure, transform it first into matrix multiplication form!\n",
        "\n",
        "The optimization objective is given by:\n",
        "$\n",
        "\\begin{align}\n",
        "  \\max_{\\boldsymbol{a}}\\widetilde{L}(\\mathbf a,\\widehat{\\mathbf a})\n",
        "\\end{align}\n",
        "subject to\n",
        "\\begin{eqnarray*}\n",
        "  0 \\leqslant a_n \\leqslant C\\\\\n",
        "  0 \\leqslant \\widehat a_n \\leqslant C\n",
        "\\end{eqnarray*}\n",
        "$\n",
        "\n",
        "Once, we have found the optimum $\\boldsymbol{a}$, the prediction function of the SVR is given by\n",
        "\\begin{equation}\n",
        "y(\\mathbf x) = \\sum_{n=1}^N (a_n- \\widehat a _n)k (\\mathbf x, \\mathbf x _n) +b\n",
        "\\end{equation}\n",
        "where $b \\in \\mathbb{R}$ is the bias parameter.\n",
        "\n",
        "We can estimate $b$ by considering a data point for which $0 < a_n < C$, which must have $\\xi_n = 0$. Therefore this point must satisfy $\\epsilon + y_n - t_n = 0$.\n",
        "\\begin{equation}\n",
        "b = \\frac{1}{N_\\mathcal{M}} \\sum_{n \\in \\mathcal{M}} \\left( t_n - \\epsilon - \\sum_{m \\in \\mathcal{S}} (a_m- \\widehat a _m)k (\\mathbf x_n, \\mathbf x _m)\\right).\n",
        "\\end{equation}\n",
        "Analogous results can be obtained by considering a point for which $0 < \\widehat a_n < C$. $\\mathcal{S} \\subseteq \\{1, \\dots, N\\}$ denotes the set of support vectors and $\\mathcal{M} \\subseteq \\{1, \\dots, N\\}$ denotes the set of support vectors lying\n",
        "on the margin with $N_\\mathcal{M} = |\\mathcal{M}|$.\n",
        "\n",
        "> Below, implement a SVR for a simple regression problem by solving the dual problem above.\n",
        "> For optimization make use of `scipy` and its [Optimization Module](https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html#sequential-least-squares-programming-slsqp-algorithm-method-slsqp)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pV-pWnm0lXfg"
      },
      "outputs": [],
      "source": [
        "class RBFKernel:\n",
        "    def __init__(self, gamma=1):\n",
        "        \"\"\"Computes RBF kernel matrix between X_1 and X_2.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Hyperparameter of RBF kernel.\n",
        "        \"\"\"\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "    def __call__(self, X_1, X_2):\n",
        "        \"\"\"Computes the kernel matrix.\n",
        "\n",
        "        Args:\n",
        "            X_1 (array-like): Input samples in shape (N, D).\n",
        "            X_2 (array-like): Input samples in shape (N, D).\n",
        "\n",
        "        Returns:\n",
        "            ndarray: Kernel matrix of shape shape (N, M)\n",
        "        \"\"\"\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IOpVOWzlXfg"
      },
      "outputs": [],
      "source": [
        "class SVR:\n",
        "    def __init__(self, kernel_func, eps=0.2, C=1.0, random_state=42):\n",
        "        \"\"\"Implementation of a C-SVM for regression.\n",
        "        Args:\n",
        "            C (float): Regularization parameter. The strength of the regularization is inversely\n",
        "                proportional to C. Must be strictly positive. (default=1.0)\n",
        "            eps (float): ...\n",
        "            kernel_func (callable): Specifies the kernel type to be used in the algorithm.\n",
        "            random_state (int): Random state to ensure reproducibility when initializing  a values.\n",
        "        \"\"\"\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "\n",
        "    def fit(self, X, t):\n",
        "        \"\"\"Fit the SVM model according to the given training data.\n",
        "\n",
        "        Args:\n",
        "            X (array-like): Training samples of shape (N, D).\n",
        "            t (array-like): Training targets of shape (N).\n",
        "\n",
        "        Returns:\n",
        "            self: The fitted SVM object.\n",
        "        \"\"\"\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "        # Optimization\n",
        "        # Step 1: Define the loss function and its gradient.\n",
        "        def loss(a):\n",
        "            # Compute loss for given a.\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "        def jac(a):\n",
        "            # Compute gradient of loss function w.r.t. a.\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "        # Step 2: Define the Constraints.\n",
        "        # We need to write the contraints in matrix notation:\n",
        "        # - for inequalities: Ax <= b\n",
        "        # - for eqalities cx = d\n",
        "        # Note that x = a in our example.\n",
        "        # 'fun' in the constraints needs to be adapted such that\n",
        "        # 0 <= lambda a: ....\n",
        "\n",
        "        # Set up the constraints:\n",
        "        # Example: {'type': 'eq', 'fun': lambda a: a**2, 'jac': lambda a: 2*a}\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "        # Optimize the a vector.\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "        self.a_ = minimize(loss, a0, jac=jac, constraints=constraints, method='SLSQP').x\n",
        "        self.a_[np.isclose(self.a_, 0)] = 0  # zero out nearly zeros\n",
        "        self.a_[np.isclose(self.a_, self.C)] = self.C  # round the ones that are nearly C\n",
        "\n",
        "\n",
        "        # Determine indices of support vectors.\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "        # Determine indices of support vectors that lie on the margin.\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "        # Determine bias parameter.\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "        # Store support vectors including their targets and a.\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Perform regression on samples in X.\n",
        "\n",
        "        Args:\n",
        "            X (array-like): Input samples whose targets are to be predicted.\n",
        "\n",
        "        Returns:\n",
        "            y (array-like): Predicted target of samples in X.\n",
        "        \"\"\"\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHz1Le5KlXfh"
      },
      "source": [
        "> Train the SVR on the given dataset and plot its support vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzoqv15blXfi"
      },
      "outputs": [],
      "source": [
        "X, y = get_simple()\n",
        "\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
