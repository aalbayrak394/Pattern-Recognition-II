{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJyj4f61P-Gd"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEq25m0eP-Ge"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.datasets import make_circles, make_blobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s49h1RXOP-Gf"
      },
      "outputs": [],
      "source": [
        "def get_simple():\n",
        "    X, y = make_blobs(centers=[[-1.5, -1.5], [1.5, 1.5]], n_samples=100, random_state=42)\n",
        "    y[y == 0] = -1\n",
        "    return X, y\n",
        "\n",
        "def get_difficult():\n",
        "    X, y = make_circles(n_samples=200, noise=0.2, factor=0.5, random_state=42)\n",
        "    y[y == 0] = -1\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2k0FwsVP-Gf"
      },
      "outputs": [],
      "source": [
        "def make_meshgrid(x, y, h=.02):\n",
        "    x_min, x_max = np.min(x) - 0.1 * np.abs(np.min(x)), np.max(x) + 0.1 * np.abs(np.max(x))\n",
        "    y_min, y_max = np.min(y) - 0.1 * np.abs(np.min(y)), np.max(y) + 0.1 * np.abs(np.max(y))\n",
        "    xx, yy, = np.mgrid[x_min:x_max:h, y_min:y_max:h]\n",
        "    return xx, yy\n",
        "\n",
        "def plot_contours(ax, svm, xx, yy, **params):\n",
        "    # Mesh samples.\n",
        "    X_mesh = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "    # Compute decision values for mesh samples.\n",
        "    Z = svm.decision_function(X_mesh)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot contours.\n",
        "    out = ax.contourf(xx, yy, Z, **params)\n",
        "\n",
        "    # Plot decision boundary.\n",
        "    out = ax.contour(xx, yy, Z, [-.1, .1], vmin=-.012, vmax=.012, linewidths=[2, 2])\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEeG1iA9P-Gg"
      },
      "source": [
        "### **1. Training of SVMs via Constrained Optimization** <a class=\"anchor\" id=\"optim\"></a>\n",
        "\n",
        "Throughout this notebook, we assume $\\mathbf{X} \\in \\mathbb{R}^{N \\times D}$ as $N \\times D$ matrix of training examples and $\\mathbf{t} \\in \\{-1, +1\\}^N$ as $N$-dimensional vector of training targets (i.e., **binary** class labels).\n",
        "To express the dual SVM in standard form, we first have to express the kernel matrix $K \\in \\mathbb{R}^{NxN}$ such that each entry is $K_{ij} = k(\\mathbf{x}_i , \\mathbf{x}_j)$. For convenience of notation, we introduce a $N \\times N$ matrix with zeros everywhere except on the diagonal, where we store the labels, that is, $\\mathbf{T} = \\text{diag}(\\mathbf{t})$.\n",
        "\n",
        "The dual form of the SVM was introduced as:\n",
        "\\begin{align}\n",
        "   \\widetilde{L}(\\mathbf{a}) = \\sum\\limits_{n=1}^N a_n - \\frac{1}{2} \\sum\\limits_{n=1}^N \\sum\\limits_{m=1}^N a_n a_m t_n t_m k (\\mathbf x _n,\\mathbf x _m) \\\\\n",
        "\\end{align}\n",
        "\n",
        "To simplify the mathematical procedure, we transform it into matrix multiplication form:\n",
        "\\begin{align*}\n",
        "\\widetilde{L}(\\mathbf{a}) = \\mathbf{1}^\\mathrm{T}_{N} \\boldsymbol{a} - \\frac{1}{2} \\boldsymbol{a}^\\mathrm{T} \\mathbf{T} \\mathbf{K} \\mathbf{T} \\boldsymbol{a}\n",
        "\\end{align*}\n",
        "where  $\\mathbf{1}_N$ denotes an $N$-dimensional vector of ones.\n",
        "\n",
        "The optimization objective is given by:\n",
        "\\begin{align}\n",
        "\\max_{\\boldsymbol{a}}\\widetilde{L}(\\mathbf{a})\n",
        "\\end{align}\n",
        "subject to\n",
        "\\begin{eqnarray*}\n",
        "0 \\leqslant a_n \\leqslant C, \\\\\n",
        "\\sum\\limits_{n=1}^N a_n t_n = 0.\n",
        "\\end{eqnarray*}\n",
        "\n",
        "Since there are many different possible views of the SVM, there are many approaches for solving the resulting optimization\n",
        "problem.  The approach presented and to be implemented here, expressing the SVM problem in standard convex optimization form, is not often used in practice. However, more sophisticated optimization algorithms would be out of the scope of this exercise.\n",
        "\n",
        "Once, we have found the optimum $\\boldsymbol{a}$, the decision function of the SVM is given by\n",
        "\\begin{equation}\n",
        "d(\\mathbf{x}) = \\sum_{n=1}^N a_n y_n k(\\mathbf{x}_n, \\mathbf{x}) + b,\n",
        "\\end{equation}\n",
        "where $b \\in \\mathbb{R}$ is the bias parameter. We can compute it according to\n",
        "\\begin{equation}\n",
        "b = \\frac{1}{N_\\mathcal{M}} \\sum_{n \\in \\mathcal{M}} \\left(y_n - \\sum_{m \\in \\mathcal{S}} a_m y_m k(\\mathbf{x}_n, \\mathbf{x}_m)\\right),\n",
        "\\end{equation}\n",
        "where $\\mathcal{S} \\subseteq \\{1, \\dots, N\\}$ denotes the set of support vectors and $\\mathcal{M} \\subseteq \\{1, \\dots, N\\}$ denotes the set of support vectors lying\n",
        "on the margin with $N_\\mathcal{M} = |\\mathcal{M}|$.\n",
        "\n",
        "Finally, we can make predictions according to\n",
        "\\begin{equation}\n",
        "y(\\mathbf{x}) = \\text{sign}(d(\\mathbf{x})).\n",
        "\\end{equation}\n",
        "\n",
        "> Below, implement a $C$-SVM for binary classification by solving the dual problem above.\n",
        "> For optimization make use of `scipy` and its [Optimization Module](https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html#sequential-least-squares-programming-slsqp-algorithm-method-slsqp)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7PLI6Y9P-Gg"
      },
      "outputs": [],
      "source": [
        "class LinearKernel:\n",
        "    def __call__(self, X_1, X_2):\n",
        "        \"\"\"Computes linear kernel matrix between X_1 and X_2.\n",
        "\n",
        "        Args:\n",
        "            X_1 (array-like): Input samples in shape (N, D).\n",
        "            X_2 (array-like): Input samples in shape (N, D).\n",
        "\n",
        "        Returns:\n",
        "            ndarray: Kernel matrix of shape shape (N, M)\n",
        "        \"\"\"\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj8G7xUkP-Gh"
      },
      "outputs": [],
      "source": [
        "class RBFKernel:\n",
        "    def __init__(self, gamma=1):\n",
        "        \"\"\"Computes RBF kernel matrix between X_1 and X_2.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Hyperparameter of RBF kernel.\n",
        "        \"\"\"\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "    def __call__(self, X_1, X_2):\n",
        "        \"\"\"Computes the kernel matrix.\n",
        "\n",
        "        Args:\n",
        "            X_1 (array-like): Input samples in shape (N, D).\n",
        "            X_2 (array-like): Input samples in shape (N, D).\n",
        "\n",
        "        Returns:\n",
        "            ndarray: Kernel matrix of shape shape (N, M)\n",
        "        \"\"\"\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wchpVZxP-Gh"
      },
      "outputs": [],
      "source": [
        "class SVM:\n",
        "    def __init__(self, kernel_func, C=1.0, random_state=42):\n",
        "        \"\"\"Implementation of a C-SVM for binary classification.\n",
        "        Args:\n",
        "            C (float): Regularization parameter. The strength of the regularization is inversely\n",
        "                proportional to C. Must be strictly positive. (default=1.0)\n",
        "            kernel_func (callable): Specifies the kernel type to be used in the algorithm.\n",
        "            random_state (int): Random state to ensure reproducibility when initializing  a values.\n",
        "        \"\"\"\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "\n",
        "    def fit(self, X, t):\n",
        "        \"\"\"Fit the SVM model according to the given training data.\n",
        "\n",
        "        Args:\n",
        "            X (array-like): Training samples of shape (N, D).\n",
        "            t (array-like): Training targets of shape (N).\n",
        "\n",
        "        Returns:\n",
        "            self: The fitted SVM object.\n",
        "        \"\"\"\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "        # Optimization\n",
        "        # Step 1: Define the loss function and its gradient.\n",
        "        def loss(a):\n",
        "            # Compute loss for given a.\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "        def jac(a):\n",
        "            # Compute gradient of loss function w.r.t. a.\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "        # Step 2: Define the Constraints.\n",
        "        # We need to write the contraints in matrix notation:\n",
        "        # - for inequalities: Ax <= b\n",
        "        # - for eqalities cx = d\n",
        "        # Note that x = a in our example.\n",
        "        # 'fun' in the constraints needs to be adapted such that\n",
        "        # 0 <= lambda a: ....\n",
        "\n",
        "        # Set up the constraints:\n",
        "        # Example: {'type': 'eq', 'fun': lambda a: a**2, 'jac': lambda a: 2*a}\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "        # Optimize the a vector.\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "        self.a_ = minimize(loss, a0, jac=jac, constraints=constraints, method='SLSQP').x\n",
        "\n",
        "        # Round a values being nearly zero to zero. (for numerical stability)\n",
        "        self.a_[np.isclose(self.a_, 0)] = 0  # zero out nearly zeros\n",
        "        self.a_[np.isclose(self.a_, self.C)] = self.C  # round the ones that are nearly C\n",
        "\n",
        "        # Determine indices of support vectors.\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "        # Determine indices of support vectors that lie on the margin.\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "        # Determine bias parameter.\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "        # Store support vectors including their targets and a.\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "        return self\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        \"\"\"Evaluates the decision function for the samples in X.\n",
        "\n",
        "        Args:\n",
        "            X (array-like): Input samples for which decision function is to be evaluated.\n",
        "\n",
        "        Returns:\n",
        "            ndarray: Decision values of samples in X.\n",
        "\n",
        "        \"\"\"\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Perform classification on samples in X.\n",
        "\n",
        "        Args:\n",
        "            X (array-like): Input samples whose class labels are to be predicted.\n",
        "\n",
        "\n",
        "        Returns:\n",
        "            y (array-like): Predicted class labels of samples in X.\n",
        "        \"\"\"\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ2dSdLXP-Gi"
      },
      "source": [
        "### **2. Classification of Linearly Separable Data Set** <a class=\"anchor\" id=\"linear\"></a>\n",
        "In a first step, we want to investigate whether our $C$-SVM with a linear kernel is able to separate the following data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M5qdZrrP-Gi"
      },
      "outputs": [],
      "source": [
        "X, y = get_simple()\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohe830usP-Gi"
      },
      "source": [
        "> Plot the decision boundary and support vectors (use `mesh_grid` and `plot_contours` which were specified above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVywLbjCP-Gi"
      },
      "outputs": [],
      "source": [
        "####################\n",
        "# Your Code Here   #\n",
        "####################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKqM11GlP-Gj"
      },
      "source": [
        "### **3. Classification of Non-linearly Separable Data Set** <a class=\"anchor\" id=\"non-linear\"></a>\n",
        "\n",
        "Before, we showed the predictive performance of our SVM on a linearly separable data set. In the second step, we generate a more difficult one instead as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HPxJYdfP-Gj"
      },
      "outputs": [],
      "source": [
        "X, y = get_difficult()\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NDfK57PP-Gj"
      },
      "source": [
        "We use the same setting as before to visualize the predictions of our SVM. However, this time with an RBF kernel for the non-linearly separable data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPicHAGwP-Gj"
      },
      "outputs": [],
      "source": [
        "####################\n",
        "# Your Code Here   #\n",
        "####################"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}